# Reward‚ÄëShaping‚Äëfor‚ÄëRobotic‚ÄëControl  
*A comparative study of reward‚Äëshaping techniques for PPO on Reacher‚Äëv5*

[![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/)  
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## üöÄ Project Overview
This repo investigates **how different reward‚Äëshaping strategies influence learning speed and final performance** of a PPO agent controlling the 2‚ÄëDoF **Reacher‚Äëv5** robotic arm (Gymnasium).  
We compare:

| Label | Strategy | œï‚Äëfunction (potential) | Notes |
|-------|----------|------------------------|-------|
| **B‚Äë0** | Baseline | ‚Äî | raw env reward |
| **Œî‚Äë1** | PBRS‚ÄëL2 | \(-‚Äñ\text{tip}‚àí\text{target}‚Äñ\) |
| **Œî‚Äë2** | PBRS‚ÄëL2¬≤ | \(-‚Äñ\text{tip}‚àí\text{target}‚Äñ^{2}\) |
| **Œî‚Äë3** | PBRS‚ÄëDecay | Œî‚Äë1 √ó Œ±(t) (linear decay ‚Üí¬†0) |
| *(opt.)* | Reward‚ÄëScale | \(k¬∑r\) | ablation |

---

## üóÇÔ∏è Directory Structure
```
Reward-shaping-for-robotic-control/
‚îÇ
‚îú‚îÄ rl_modules/                # importable package
‚îÇ   ‚îú‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ potential_functions.py # registry of POTENTIAL functions L2, L2_squared
‚îÇ   ‚îÇ   ‚îî‚îÄ utils.py           # make_reacher_env(), train(), eval_agent(), GAE, logging
‚îÇ   ‚îî‚îÄ models/
‚îÇ       ‚îú‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ networks.py        # Actor‚ÄëCritic MLP
‚îÇ       ‚îî‚îÄ ppo_agent.py       # PPO with optional shaping (none, L2, L2_squared, decay)
‚îÇ
‚îú‚îÄ data/                      # logs (json), rollouts, models (.pt)
‚îú‚îÄ pilot_none/
‚îÇ   ‚îú‚îÄ ppo_model.pt
‚îÇ   ‚îî‚îÄ train_log.json
‚îú‚îÄ pilot_l2/
‚îÇ   ‚îú‚îÄ ppo_model.pt
‚îÇ   ‚îî‚îÄ train_log.json
‚îú‚îÄ ...                    # some more data stuff
‚îÇ
‚îú‚îÄ train.py               # command‚Äëline training entry
‚îú‚îÄ run_rollout.py         # visualize a saved model
‚îú‚îÄ plot_results.ipynb     # analysis / figures
‚îú‚îÄ plot_training.py       # plot script for training data
‚îÇ
‚îÇ
‚îî‚îÄ README.md
```

---

## ‚öôÔ∏è Installation

```bash
# 1. (Recommended) create a fresh env
conda create -n rl_env python=3.11 -y
conda activate rl_env

# 2. Install dependencies
pip install -r requirements.txt
#  gymnasium[box2d] for Reacher‚Äëv5, torch, wandb, imageio, etc.
```

> **Colab**: open `colab_train.ipynb`, pip‚Äëinstall the same `requirements.txt`, then `!python scripts/train.py --variant l2`.

---

## Quick Start

### 1. Baseline PPO
```bash
python scripts/train.py --variant none --timesteps 100000 --run_name baseline
```

### 2. Potential‚Äëbased L2 shaping
```bash
python scripts/train.py --variant l2 --timesteps 100000 --run_name pbrs_l2
```

### 3. Rollout a trained model
```bash
python scripts/run_rollout.py
```
Creates `rollout.gif` and `rollout.npz`.

---

## üõ†Ô∏è CLI Options (`train.py`)
| flag | default | description |
|------|---------|-------------|
| `--variant` | `none` | `none`, `l2`, `l2sq`, `decay`, `scale` |
| `--timesteps` | `500000` | total env steps |
| `--decay_steps` | `300k` | when Œ±(t)=0 (for `decay`) |
| `--scale_k` | `2.0` | reward multiplier (`scale`) |
| `--run_name` | auto | sub‚Äëfolder in `data/` |

---


Questions or contributions? Feel free to open an Issue or PR!
