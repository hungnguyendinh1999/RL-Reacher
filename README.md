# Rewardâ€‘Shapingâ€‘forâ€‘Roboticâ€‘Control  
*A comparative study of rewardâ€‘shaping techniques for PPO on Reacherâ€‘v5*

[![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/)  
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## ğŸš€ Project Overview
This repo investigates how different **rewardâ€‘shaping strategies** influence learning speed and final performance of a PPO agent controlling the 2â€‘DoF **Reacherâ€‘v5** robotic arm ([Gymnasium](https://gymnasium.farama.org/environments/mujoco/reacher/)).  
We compare:

| Label | Strategy | $\phi$â€‘function (potential) | Notes |
|-------|----------|------------------------|-------|
| **Bâ€‘0** | Baseline | â€” | raw env reward |
| **$\Deltaâ€‘1$** | PBRSâ€‘L2 | $-â€–\text{tip}âˆ’\text{target}â€–$ |
| **$\Deltaâ€‘2$** | PBRSâ€‘L2Â² | $-â€–\text{tip}âˆ’\text{target}â€–^{2}$ |
| **$\Deltaâ€‘3$** | PBRSâ€‘Decay | $\Deltaâ€‘1 Ã— \alpha(t)$ (linear decay $\rightarrow$Â 0) |
| *(todo.)* | Rewardâ€‘Scale | $k \cdot r$ | ablation |

---

## ğŸ—‚ï¸ Directory Structure
```
Reward-shaping-for-robotic-control/
â”‚
â”œâ”€ rl_modules/                # importable package
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ core/
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ potential_functions.py # registry of POTENTIAL functions L2, L2_squared
â”‚   â”‚   â””â”€ utils.py           # make_reacher_env(), train(), eval_agent(), GAE, logging
â”‚   â””â”€ models/
â”‚       â”œâ”€ __init__.py
â”‚       â”œâ”€ networks.py        # Actorâ€‘Critic MLP
â”‚       â””â”€ ppo_agent.py       # PPO with optional shaping (none, L2, L2_squared, decay)
â”‚
â”œâ”€ data/                      # logs (json), rollouts, models (.pt)
â”œâ”€ pilot_none/
â”‚   â”œâ”€ ppo_model.pt
â”‚   â””â”€ train_log.json
â”œâ”€ pilot_l2/
â”‚   â”œâ”€ ppo_model.pt
â”‚   â””â”€ train_log.json
â”œâ”€ ...                    # some more data stuff
â”‚
â”œâ”€ train.py               # commandâ€‘line training entry
â”œâ”€ run_rollout.py         # visualize a saved model
â”œâ”€ plot_results.ipynb     # analysis / figures
â”œâ”€ plot_training.py       # plot script for training data
â”‚
â”‚
â””â”€ README.md
```

---

## âš™ï¸ Installation

```bash
# 1. (Recommended) create a fresh env
conda create -n rl_env python=3.11 -y
conda activate rl_env

# 2. Install dependencies
pip install -r requirements.txt
#  gymnasium[mujoco] for Reacherâ€‘v5, torch, imageio, etc.
```

---

## Quick Start

### 1. Baseline PPO
```bash
python train.py --variant none --timesteps 100000 --run_name baseline
```

### 2. Potentialâ€‘based L2 shaping
```bash
python train.py --variant l2 --timesteps 100000 --run_name pbrs_l2
```

### 3. Rollout a trained model
```bash
python run_rollout.py
```
Creates `rollout.gif` and `rollout.npz`.

---

## ğŸ› ï¸ CLI Options (`train.py`)
| flag | default | description |
|------|---------|-------------|
| `--variant` | `none` | `none`, `l2`, `l2sq`, `decay`, `scale` |
| `--timesteps` | `500000` | total env steps |
| `--decay_steps` | `300k` | when $\alpha(t)=0$ (for `decay`) |
| `--scale_k` | `2.0` | reward multiplier (`scale`) |
| `--run_name` | auto | subâ€‘folder in `data/` |

---


Questions or contributions? Feel free to open an Issue or PR!
